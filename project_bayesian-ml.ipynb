{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 style=\"text-align: center; font-size: 50px;font-weight: bold;\"> Bayesian Machine Learning\n",
    "   </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 style=\"text-align: center; font-size: 25px;font-weight: bold;\"> Project : Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural\n",
    " Networks with Many More Parameters than Training Data </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.392499Z",
     "iopub.status.busy": "2025-03-19T11:24:08.392137Z",
     "iopub.status.idle": "2025-03-19T11:24:08.397401Z",
     "shell.execute_reply": "2025-03-19T11:24:08.396362Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.392469Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "import time as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.398690Z",
     "iopub.status.busy": "2025-03-19T11:24:08.398400Z",
     "iopub.status.idle": "2025-03-19T11:24:08.505520Z",
     "shell.execute_reply": "2025-03-19T11:24:08.504728Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.398668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "# Download and load the dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, range(55000))\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.507076Z",
     "iopub.status.busy": "2025-03-19T11:24:08.506868Z",
     "iopub.status.idle": "2025-03-19T11:24:08.535710Z",
     "shell.execute_reply": "2025-03-19T11:24:08.534914Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.507059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "images, labels = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.537454Z",
     "iopub.status.busy": "2025-03-19T11:24:08.536944Z",
     "iopub.status.idle": "2025-03-19T11:24:08.541314Z",
     "shell.execute_reply": "2025-03-19T11:24:08.540654Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.537434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.542270Z",
     "iopub.status.busy": "2025-03-19T11:24:08.542001Z",
     "iopub.status.idle": "2025-03-19T11:24:08.696029Z",
     "shell.execute_reply": "2025-03-19T11:24:08.695411Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.542242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(images[0,0,:,:])\n",
    "print(torch.mean(images[0,0,:,:]), torch.max(images[0,0,:,:]), torch.min(images[0,0,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.697100Z",
     "iopub.status.busy": "2025-03-19T11:24:08.696799Z",
     "iopub.status.idle": "2025-03-19T11:24:08.704000Z",
     "shell.execute_reply": "2025-03-19T11:24:08.703256Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.697071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, nb_nodes, nb_hidden_layers, classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [torch.nn.Flatten(), nn.Linear(28 * 28, nb_nodes), nn.ReLU()]\n",
    "\n",
    "        for _ in range(nb_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(nb_nodes, nb_nodes))\n",
    "            layers.append(nn.ReLU())\n",
    "            # layers.append(nn.Dropout(p=0.4))\n",
    "\n",
    "        layers.append(nn.Linear(nb_nodes, classes))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.initialize_weights()   # Initialize weights after layer creation\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "    def truncated_normal_(self, tensor, mean=0, std=0.04, low=-2, high=2):\n",
    "        \"\"\"Truncated normal initialization for weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            tensor.normal_(mean, std)\n",
    "            tensor.clamp_(low*std, high*std)  # Truncate values to [-2*sigma, 2*sigma]\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize weights and biases as per the given specifications.\"\"\"\n",
    "        linear_layer = 0\n",
    "        for  m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                self.truncated_normal_(m.weight, mean=0, std=0.04, low=-2, high=2)\n",
    "                \n",
    "                if linear_layer == 0:  # First layer after input\n",
    "                    init.constant_(m.bias, 0.1)\n",
    "                else:\n",
    "                    init.constant_(m.bias, 0)\n",
    "                linear_layer +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.704868Z",
     "iopub.status.busy": "2025-03-19T11:24:08.704642Z",
     "iopub.status.idle": "2025-03-19T11:24:08.724277Z",
     "shell.execute_reply": "2025-03-19T11:24:08.723638Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.704849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "epochs = 200\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.726456Z",
     "iopub.status.busy": "2025-03-19T11:24:08.726237Z",
     "iopub.status.idle": "2025-03-19T11:24:08.754460Z",
     "shell.execute_reply": "2025-03-19T11:24:08.753698Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.726438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = MLP(nb_nodes=600, nb_hidden_layers=2).to(device)\n",
    "model = MLP(nb_nodes=600, nb_hidden_layers=1).to(device)\n",
    "print(model)\n",
    "\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr = lr, momentum=0.9, eps=1e-5)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.755881Z",
     "iopub.status.busy": "2025-03-19T11:24:08.755671Z",
     "iopub.status.idle": "2025-03-19T11:24:08.759813Z",
     "shell.execute_reply": "2025-03-19T11:24:08.759005Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.755863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_weights_and_biases(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"Layer: {name} | Weights:\\n{param.data}\\n\")\n",
    "        elif \"bias\" in name:\n",
    "            print(f\"Layer: {name} | Biases:\\n{param.data}\\n\")\n",
    "\n",
    "# print_weights_and_biases(model)\n",
    "# path = '/kaggle/working/'\n",
    "path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.760795Z",
     "iopub.status.busy": "2025-03-19T11:24:08.760539Z",
     "iopub.status.idle": "2025-03-19T11:24:08.780717Z",
     "shell.execute_reply": "2025-03-19T11:24:08.779853Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.760766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def plot_training_history(history_train_loss, history_val_loss):\n",
    "    epochs = range(1, len(history_train_loss) + 1)\n",
    "\n",
    "    # figure avec deux sous-graphes côte à côte\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Graphique pour la perte\n",
    "    plt.semilogy(epochs, history_train_loss, label='Train Loss', marker='o')\n",
    "    plt.semilogy(epochs, history_val_loss, label='Validation Loss', marker='o')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def label_modif(label):\n",
    "    \"\"\" Changes the labels of the MNIST dataset to transform it into a binary\n",
    "    classification problem by mapping numbers {0,.....,4} to label 1 and {5,......,9} to label -1\"\"\"\n",
    "    # print(\"label check before\", label)\n",
    "    label[label<=4] = 1\n",
    "    label[label >=5] = -1\n",
    "    # print(\"label check after\", label)\n",
    "    \n",
    "    return label\n",
    "\n",
    "def logistic_loss(labels, outputs):\n",
    "    loss = (1/(np.log(2)))*torch.log(1+torch.exp(-labels*outputs))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "def training_pipeline(epochs, model, train_loader, test_loader, optimizer, patience):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_train_tab = []\n",
    "    loss_valid_tab = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    model.to(device)\n",
    "    print(\"Module sélectionné pour l'entraînement :\", device)\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for k, (image_train, label_train) in enumerate(train_loader):\n",
    "            label_train = label_modif(label_train)\n",
    "            image_train, label_train = image_train.to(device), label_train.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image_train)\n",
    "            loss = logistic_loss(label_train, output.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            # Prediction: sign of output as predicted label (assumes output is a real number)\n",
    "            predictions = torch.sign(output.squeeze(1))\n",
    "            total_train_correct += (predictions == label_train).sum().item()\n",
    "            total_train_samples += label_train.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = total_train_correct / total_train_samples\n",
    "        loss_train_tab.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for k, (image_test, label_test) in enumerate(test_loader):\n",
    "                label_test = label_modif(label_test)\n",
    "                image_test, label_test = image_test.to(device), label_test.to(device).float()\n",
    "                output = model(image_test)\n",
    "                loss = logistic_loss(label_test, output.squeeze(1))\n",
    "                total_val_loss += loss.item()\n",
    "                predictions = torch.sign(output.squeeze(1))\n",
    "                total_val_correct += (predictions == label_test).sum().item()\n",
    "                total_val_samples += label_test.size(0)\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        val_accuracy = total_val_correct / total_val_samples\n",
    "        loss_valid_tab.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model)  # Sauvegarde du meilleur modèle\n",
    "            torch.save(best_model.state_dict(), path + 'model_init.pt')\n",
    "            \n",
    "            epochs_no_improve = 0  # Reset du compteur\n",
    "            print(f\"A new best model has been found with validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            plot_training_history(loss_train_tab, loss_valid_tab)\n",
    "            \n",
    "    plot_training_history(loss_train_tab, loss_valid_tab)\n",
    "    return best_model, loss_valid_tab\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:24:08.781786Z",
     "iopub.status.busy": "2025-03-19T11:24:08.781524Z",
     "iopub.status.idle": "2025-03-19T11:28:41.151490Z",
     "shell.execute_reply": "2025-03-19T11:28:41.150678Z",
     "shell.execute_reply.started": "2025-03-19T11:24:08.781757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model, loss_validation = training_pipeline(epochs=20, model=model, train_loader=train_loader, test_loader=test_loader, optimizer=optimizer, patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAC BOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:28:41.152550Z",
     "iopub.status.busy": "2025-03-19T11:28:41.152299Z",
     "iopub.status.idle": "2025-03-19T11:28:41.159518Z",
     "shell.execute_reply": "2025-03-19T11:28:41.158591Z",
     "shell.execute_reply.started": "2025-03-19T11:28:41.152530Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Linear Layer\n",
    "\n",
    "class StochLinear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, \n",
    "                 init_inv_softplus_sigma=-3.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight_mu = torch.nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.weight_ispsigma = torch.nn.Parameter(torch.empty(out_features, in_features).fill_(\n",
    "                                                                                init_inv_softplus_sigma))\n",
    "        if bias:\n",
    "            self.bias_mu = torch.nn.Parameter(torch.zeros(out_features))\n",
    "            self.bias_ispsigma = torch.nn.Parameter(torch.empty(out_features).fill_(init_inv_softplus_sigma))\n",
    "        self.eps = eps\n",
    "        self.with_bias = bias\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight = self.weight_mu + torch.randn_like(self.weight_mu) * self.weight_sigma\n",
    "        if self.with_bias:\n",
    "            bias = self.bias_mu + torch.randn_like(self.bias_mu) * self.bias_sigma\n",
    "        else:\n",
    "            bias = None\n",
    "        return torch.nn.functional.linear(x, weight, bias)\n",
    "    \n",
    "    def copy_weights(self, linear_layer):\n",
    "        self.weight_mu.detach().copy_(linear_layer.weight)\n",
    "        if self.with_bias:\n",
    "            self.bias_mu.detach().copy_(linear_layer.bias)\n",
    "            \n",
    "    @property\n",
    "    def weight_sigma(self):\n",
    "        return torch.nn.functional.softplus(self.weight_ispsigma) + self.eps\n",
    "    \n",
    "    @property\n",
    "    def bias_sigma(self):\n",
    "        return torch.nn.functional.softplus(self.bias_ispsigma) + self.eps\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:28:41.160454Z",
     "iopub.status.busy": "2025-03-19T11:28:41.160200Z",
     "iopub.status.idle": "2025-03-19T11:28:41.181528Z",
     "shell.execute_reply": "2025-03-19T11:28:41.180797Z",
     "shell.execute_reply.started": "2025-03-19T11:28:41.160434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Neural Network\n",
    "\n",
    "class StochaNN(nn.Module):\n",
    "    def __init__(self, nb_nodes, nb_hidden_layers, classes=1, isp_sigma=-3.0):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [torch.nn.Flatten(), StochLinear(28*28, nb_nodes, init_inv_softplus_sigma=isp_sigma), nn.ReLU()]\n",
    "\n",
    "        for _ in range(nb_hidden_layers - 1):\n",
    "            layers.append(StochLinear(nb_nodes, 600, init_inv_softplus_sigma=isp_sigma))\n",
    "            layers.append(nn.ReLU())\n",
    "            # layers.append(nn.Dropout(p=0.4))\n",
    "\n",
    "        layers.append(StochLinear(nb_nodes, classes, init_inv_softplus_sigma=isp_sigma))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:57:47.998074Z",
     "iopub.status.busy": "2025-03-19T11:57:47.997528Z",
     "iopub.status.idle": "2025-03-19T11:57:48.013292Z",
     "shell.execute_reply": "2025-03-19T11:57:48.012380Z",
     "shell.execute_reply.started": "2025-03-19T11:57:47.998035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Approximate PAC-Bayes bound\n",
    "\n",
    "def kl_bernoulli(q, p):\n",
    "    return q * math.log(q / p)+ (1 - q) * math.log((1 - q) / (1 - p))\n",
    "    \n",
    "def get_approx_bound(b_re, acc, res=1000):\n",
    "    ts = np.linspace(0., 1., res)\n",
    "    print(\"acc\", acc)\n",
    "    kls = [kl_bernoulli(1 - acc, t) for t in ts]\n",
    "    idx = np.argmin(np.abs(np.array(kls) - b_re))\n",
    "    return ts[idx]\n",
    "\n",
    "# Training\n",
    "def train_epoch(loader, model, loss_fn, optimizer, device=device):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_train_samples = 0\n",
    "    total_train_correct = 0\n",
    "    acc = 0\n",
    "\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "    \n",
    "        inputs = inputs.to(device)\n",
    "        targets = label_modif(targets)\n",
    "        targets = targets.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss, _, _ = loss_fn(model, inputs, targets)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        total_train_samples += batch_size\n",
    "        predictions = torch.sign(outputs.squeeze(1))\n",
    "        total_train_correct += (predictions == targets).sum().item()\n",
    "\n",
    "        total_train_samples += 1\n",
    "    acc = acc / len(loader)\n",
    "    avg_loss = running_loss / total_samples\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "    \n",
    "    return {\"loss\": avg_loss, \"stats\": {'accuracy': train_accuracy}}\n",
    "\n",
    "\n",
    "def eval_model(loader, model, loss_fn, device=device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = label_modif(targets)\n",
    "            targets = targets.to(device).float()\n",
    "            outputs = model(inputs)\n",
    "            loss, _, _ = loss_fn(model, inputs, targets)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            predictions = torch.sign(outputs.squeeze(1))\n",
    "            total_correct += (predictions == targets).sum().item()\n",
    "        \n",
    "        acc = acc / len(loader)\n",
    "        avg_loss = running_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "    return {\"loss\": avg_loss, \"stats\": {'accuracy': accuracy}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:57:48.190956Z",
     "iopub.status.busy": "2025-03-19T11:57:48.190675Z",
     "iopub.status.idle": "2025-03-19T11:57:48.201883Z",
     "shell.execute_reply": "2025-03-19T11:57:48.200828Z",
     "shell.execute_reply.started": "2025-03-19T11:57:48.190934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PACBound:\n",
    "    def __init__(self, prior_mean, lambda_init, likelihood_fn, num_data, delta=0.025, b=100, c=0.1):\n",
    "        \n",
    "        self.prior_mu = prior_mean.to(device)\n",
    "        self.log_lambda_ = torch.tensor([math.log(lambda_init)],\n",
    "                                   requires_grad=True, device=device)\n",
    "        self.likelihood_fn = likelihood_fn\n",
    "        self.num_data = torch.tensor([num_data], requires_grad=False, \n",
    "                                     dtype=float, device=device)\n",
    "        self.delta = delta\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        \n",
    "    def kl(self, stoch_network):\n",
    "        mu_net, sigma_net = self._get_mu_sigma(stoch_network.layers)\n",
    "        prior_sigma = torch.ones_like(sigma_net, device=sigma_net.device) * self.prior_sigma.cuda()\n",
    "\n",
    "        return self._kl_gaussians(mu_net, sigma_net, self.prior_mu, prior_sigma)\n",
    "    \n",
    "    def train_bound(self, stoch_network, x, y):\n",
    "        preds = stoch_network(x).flatten()\n",
    "        acc = (preds * y > 0).float().mean()\n",
    "        likelihood = self.likelihood_fn(preds, y)\n",
    "        regularizer =  torch.sqrt(self.B_RE(stoch_network) / 2)\n",
    "        # print(likelihood, regularizer)\n",
    "        return likelihood + regularizer, preds, \\\n",
    "               {'accuracy': acc, 'regularizer': regularizer}\n",
    "        \n",
    "    def B_RE(self, stoch_network):\n",
    "        kl = self.kl(stoch_network)\n",
    "        return (kl + \n",
    "                2 * torch.log(torch.abs(self.b * (math.log(self.c) - self.log_lambda_))) + \n",
    "                math.log(math.pi**2 * self.num_data / (6 * self.delta))) / (self.num_data - 1)\n",
    "        \n",
    "    @property\n",
    "    def prior_sigma(self):\n",
    "        return 1 / torch.sqrt(torch.exp(self.log_lambda_))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _kl_gaussians(mu_1, sigma_1, mu_2, sigma_2):\n",
    "        device = mu_1.device\n",
    "\n",
    "        mu_1 = mu_1.to(device)\n",
    "        mu_2 = mu_2.to(device)\n",
    "        sigma_1 = sigma_1.to(device)\n",
    "        sigma_2 = sigma_2.to(device)\n",
    "        \n",
    "        d = mu_1.numel()\n",
    "        kl = 0.5 * (torch.sum(torch.log(sigma_2) - torch.log(sigma_1)) * 2 +\n",
    "                    torch.sum(sigma_1**2 / sigma_2**2) +\n",
    "                    torch.sum((mu_2 - mu_1)**2 / sigma_2**2) - d)\n",
    "        return kl \n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_mu_sigma(stoch_network):\n",
    "        mu_net, sigma_net = [torch.cat(a) for a in \n",
    "                             zip(*[\n",
    "                                  [torch.cat([layer.weight_mu.flatten(), layer.bias_mu.flatten()]),\n",
    "                                  torch.cat([layer.weight_sigma.flatten(), layer.bias_sigma.flatten()])]\n",
    "                                  for \n",
    "                                  layer in stoch_network if\n",
    "                                  isinstance(layer, StochLinear)])\n",
    "                            ]\n",
    "        return mu_net, sigma_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:57:48.405085Z",
     "iopub.status.busy": "2025-03-19T11:57:48.404711Z",
     "iopub.status.idle": "2025-03-19T11:57:48.451098Z",
     "shell.execute_reply": "2025-03-19T11:57:48.450484Z",
     "shell.execute_reply.started": "2025-03-19T11:57:48.405055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = MLP(nb_nodes=600, nb_hidden_layers=1)\n",
    "state_dict = torch.load(path + \"model_init.pt\")\n",
    "model.load_state_dict(state_dict)\n",
    "model.cuda()\n",
    "\n",
    "prior_mu = torch.cat([p.detach().flatten() for p in model.layers.parameters()])\n",
    "pac_bound = PACBound(prior_mu, lambda_init=1e-3, likelihood_fn=logistic_loss, num_data=len(train_dataset))\n",
    "\n",
    "stoch_model = StochaNN(nb_nodes=600, nb_hidden_layers=1, isp_sigma=-2)\n",
    "print(stoch_model)\n",
    "for stoch_layer, layer in zip(stoch_model.layers, model.layers):\n",
    "    print(\"stoch\", stoch_layer, \"layer\", layer)\n",
    "    if isinstance(stoch_layer, StochLinear):\n",
    "        stoch_layer.copy_weights(layer)\n",
    "stoch_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T11:57:48.549009Z",
     "iopub.status.busy": "2025-03-19T11:57:48.548745Z",
     "iopub.status.idle": "2025-03-19T11:59:12.397999Z",
     "shell.execute_reply": "2025-03-19T11:59:12.396838Z",
     "shell.execute_reply.started": "2025-03-19T11:57:48.548989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time as ts\n",
    "\n",
    "# Number of epochs to match the paper (200,000 iterations)\n",
    "\n",
    "epochs = 1000\n",
    "initial_lr = 0.001  # First 150,000 iterations\n",
    "lr_drop_epoch = 250  # Drop learning rate at 150,000 iterations\n",
    "lr_factor = 0.1  # Reduce learning rate by lr_factor\n",
    "eval_freq = 100  # Evaluate every 100 epochs\n",
    "\n",
    "# Initialize RMSprop optimizer (with alpha=0.9 as in the paper)\n",
    "optimizer_sto = torch.optim.RMSprop([\n",
    "    {'params': stoch_model.parameters(), 'lr': initial_lr, 'alpha': 0.9}, \n",
    "    {'params': pac_bound.log_lambda_, 'lr': initial_lr}\n",
    "])\n",
    "\n",
    "lambdas, train_accs, test_accs, b_re_values, kl_values, bounds, idx = [], [], [], [], [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    _ts = ts.perf_counter()\n",
    "\n",
    "    # Drop learning rate after 250 epochs\n",
    "    if epoch == lr_drop_epoch:\n",
    "        for param_group in optimizer_sto.param_groups:\n",
    "            param_group['lr'] *= lr_factor\n",
    "    \n",
    "    # Run one training epoch\n",
    "    train_res = train_epoch(train_loader, stoch_model, pac_bound.train_bound, optimizer_sto)\n",
    "    if epoch % eval_freq == 0:\n",
    "        # Evaluate the model on the test set\n",
    "        eval_res = eval_model(test_loader, stoch_model, pac_bound.train_bound)\n",
    "        test_accs.append(eval_res['stats']['accuracy'])\n",
    "        idx.append(epoch)\n",
    "        print(f'**** Epoch {epoch + 1} / {epochs}: Test accuracy: {test_accs[-1]} ****')\n",
    "    train_accs.append(train_res['stats']['accuracy'])\n",
    "    \n",
    "    # Compute PAC-Bayes metrics\n",
    "    lambdas.append(torch.exp(pac_bound.log_lambda_).item())\n",
    "    b_re_values.append(pac_bound.B_RE(stoch_model).item())\n",
    "\n",
    "    stoch_model.eval()\n",
    "    kl = pac_bound.kl(stoch_model).item()\n",
    "    kl_values.append(kl)\n",
    "    bounds.append(get_approx_bound(b_re_values[-1], train_accs[-1]))\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} / {epochs}: Train Acc = {train_accs[-1] :.4f}, λ = {lambdas[-1] :.4f}, \"\n",
    "          f\"Bound:{bounds[-1] :.4f}, B_RE = {b_re_values[-1] :.4f}, KL = {kl_values[-1] :.4f}, Time: {ts.perf_counter() - _ts :.2f}s\")\n",
    "\n",
    "\n",
    "# Plots\n",
    "f, arr = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "arr[0].plot(train_accs, lw=2, label=\"Train Accuracy\", color=\"blue\")\n",
    "arr[0].plot(idx, test_accs, lw=2, label=\"Test Accuracy\", color=\"red\")\n",
    "arr[0].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "arr[0].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[0].legend()\n",
    "arr[0].grid()\n",
    "\n",
    "arr[1].plot(lambdas, lw=2, label=r\"$\\lambda$\", color=\"red\")\n",
    "arr[1].set_ylabel(r\"$\\lambda$\", fontsize=14)\n",
    "arr[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[1].legend()\n",
    "arr[1].grid()\n",
    "\n",
    "# Plot KL Divergence\n",
    "arr[2].plot(kl_values, lw=2, label=\"KL Divergence\", color=\"green\")\n",
    "arr[2].set_ylabel(\"KL Divergence\", fontsize=14)\n",
    "arr[2].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[2].legend()\n",
    "arr[2].grid()\n",
    "\n",
    "# Plot Bound\n",
    "arr[3].plot(bounds, lw=2, label=\"Bound\", color=\"green\")\n",
    "arr[3].set_ylabel(\"Bound\", fontsize=14)\n",
    "arr[3].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[3].legend()\n",
    "arr[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.137586Z",
     "iopub.status.idle": "2025-03-19T11:29:11.137840Z",
     "shell.execute_reply": "2025-03-19T11:29:11.137741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stoch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.138409Z",
     "iopub.status.idle": "2025-03-19T11:29:11.138696Z",
     "shell.execute_reply": "2025-03-19T11:29:11.138580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(stoch_model.state_dict(), path + \"stoch_model.pt\")\n",
    "# torch.save(pac_bound.log_lambda_, \"lambda.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.139577Z",
     "iopub.status.idle": "2025-03-19T11:29:11.139916Z",
     "shell.execute_reply": "2025-03-19T11:29:11.139786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def training_pipeline_multi(epochs, model, train_loader, test_loader, optimizer, patience):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    loss_train_tab = []\n",
    "    loss_valid_tab = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    model.to(device)\n",
    "    print(\"Module sélectionné pour l'entraînement :\", device)\n",
    "    epochs_no_improve = 0\n",
    "    criterion = torch.nn.CrossEntropyLoss()  # Using CrossEntropyLoss for multi-class classification\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for k, (image_train, label_train) in enumerate(train_loader):\n",
    "            image_train, label_train = image_train.to(device), label_train.to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image_train)  \n",
    "            loss = criterion(output, label_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Get predictions: max value index as the predicted class\n",
    "            predictions = torch.argmax(output, dim=1)  # Predictions are the class with the highest logit\n",
    "            total_train_correct += (predictions == label_train).sum().item()\n",
    "            total_train_samples += label_train.size(0)\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = total_train_correct / total_train_samples\n",
    "        loss_train_tab.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for k, (image_test, label_test) in enumerate(test_loader):\n",
    "                image_test, label_test = image_test.to(device), label_test.to(device).long() \n",
    "                output = model(image_test)\n",
    "                loss = criterion(output, label_test)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions: max value index as the predicted class\n",
    "                predictions = torch.argmax(output, dim=1)  # Predictions are the class with the highest logit\n",
    "                total_val_correct += (predictions == label_test).sum().item()\n",
    "                total_val_samples += label_test.size(0)\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        val_accuracy = total_val_correct / total_val_samples\n",
    "        loss_valid_tab.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = copy.deepcopy(model)  # Save the best model\n",
    "            epochs_no_improve = 0  # Reset the counter\n",
    "            print(f\"A new best model has been found with validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            plot_training_history(loss_train_tab, loss_valid_tab)\n",
    "            \n",
    "    plot_training_history(loss_train_tab, loss_valid_tab)\n",
    "    return best_model, loss_valid_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.140452Z",
     "iopub.status.idle": "2025-03-19T11:29:11.140811Z",
     "shell.execute_reply": "2025-03-19T11:29:11.140664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_multiclass = MLP(nb_nodes=600, nb_hidden_layers=2, classes=10).to(device)\n",
    "optimizer_multi = torch.optim.SGD(model_multiclass.parameters(), lr=0.01, momentum=0.9)\n",
    "best_model, loss_valid_tab = training_pipeline_multi(epochs=20, model=model_multiclass, train_loader=train_loader, test_loader=test_loader, optimizer=optimizer_multi, patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.141330Z",
     "iopub.status.idle": "2025-03-19T11:29:11.141550Z",
     "shell.execute_reply": "2025-03-19T11:29:11.141459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model_multiclass.state_dict(), path + \"model_init_multiclass.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.142078Z",
     "iopub.status.idle": "2025-03-19T11:29:11.142424Z",
     "shell.execute_reply": "2025-03-19T11:29:11.142279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, loss_fn, optimizer, device=device):\n",
    "    \"\"\"\n",
    "    Runs one training epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_train_samples = 0\n",
    "    total_train_correct = 0\n",
    "    acc = 0\n",
    "\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "    \n",
    "        inputs = inputs.to(device)\n",
    "        # targets = label_modif(targets)\n",
    "        targets = targets.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss, _, _ = loss_fn(model, inputs, targets)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        total_train_samples += batch_size\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        total_train_correct += (predictions == targets).sum().item()\n",
    "\n",
    "        total_train_samples += 1\n",
    "    acc = acc / len(loader)\n",
    "    avg_loss = running_loss / total_samples\n",
    "    train_accuracy = total_train_correct / total_train_samples\n",
    "    \n",
    "    return {\"loss\": avg_loss, \"stats\": {'accuracy': train_accuracy}}\n",
    "\n",
    "\n",
    "def eval_model(loader, model, loss_fn, device=device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device).long()\n",
    "            outputs = model(inputs)\n",
    "            loss, _, _ = loss_fn(model, inputs, targets)\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (predictions == targets).sum().item()\n",
    "        \n",
    "        acc = acc / len(loader)\n",
    "        avg_loss = running_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "    return {\"loss\": avg_loss, \"stats\": {'accuracy': accuracy}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.143224Z",
     "iopub.status.idle": "2025-03-19T11:29:11.143514Z",
     "shell.execute_reply": "2025-03-19T11:29:11.143409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PACBound:\n",
    "    def __init__(self, prior_mean, lambda_init, likelihood_fn, num_data, delta=0.025, b=100, c=0.1):\n",
    "        \n",
    "        self.prior_mu = prior_mean.to(device)\n",
    "        self.log_lambda_ = torch.tensor([math.log(lambda_init)],\n",
    "                                   requires_grad=True, device=device)\n",
    "        self.likelihood_fn = likelihood_fn\n",
    "        self.num_data = torch.tensor([num_data], requires_grad=False, \n",
    "                                     dtype=float, device=device)\n",
    "        self.delta = delta\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        \n",
    "    def kl(self, stoch_network):\n",
    "        mu_net, sigma_net = self._get_mu_sigma(stoch_network.layers)\n",
    "        prior_sigma = torch.ones_like(sigma_net, device=sigma_net.device) * self.prior_sigma.cuda()\n",
    "\n",
    "        return self._kl_gaussians(mu_net, sigma_net, self.prior_mu, prior_sigma)\n",
    "\n",
    "    \n",
    "    # def train_bound(self, stoch_network, x, y):\n",
    "    #     preds = stoch_network(x).flatten()\n",
    "    #     acc = (preds * y > 0).float().mean()\n",
    "    #     likelihood = self.likelihood_fn(preds, y)\n",
    "    #     regularizer =  torch.sqrt(self.B_RE(stoch_network) / 2)\n",
    "    #     # print(likelihood, regularizer)\n",
    "    #     return likelihood + regularizer, preds, \\\n",
    "    #            {'accuracy': acc, 'regularizer': regularizer}\n",
    "    \n",
    "    def train_bound(self, stoch_network, x, y):\n",
    "        # Forward pass through the stochastic network\n",
    "        preds = stoch_network(x)  # This will be of shape (batch_size, num_classes)\n",
    "        \n",
    "        # Multi-class accuracy: get the index of the maximum output (i.e., predicted class)\n",
    "        _, predicted_classes = torch.max(preds, dim=1)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        acc = (predicted_classes == y).float().mean()\n",
    "\n",
    "        # Compute likelihood (CrossEntropyLoss is used)\n",
    "        likelihood = self.likelihood_fn(preds, y.long())\n",
    "\n",
    "        # Compute the regularizer term based on the B_RE bound\n",
    "        regularizer = torch.sqrt(self.B_RE(stoch_network) / 2)\n",
    "\n",
    "        return likelihood + regularizer, preds, {'accuracy': acc, 'regularizer': regularizer}\n",
    "    \n",
    "    def B_RE(self, stoch_network):\n",
    "        kl = self.kl(stoch_network)\n",
    "        return (kl + \n",
    "                2 * torch.log(torch.abs(self.b * (math.log(self.c) - self.log_lambda_))) + \n",
    "                math.log(math.pi**2 * self.num_data / (6 * self.delta))) / (self.num_data - 1)\n",
    "        \n",
    "    @property\n",
    "    def prior_sigma(self):\n",
    "        return 1 / torch.sqrt(torch.exp(self.log_lambda_))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _kl_gaussians(mu_1, sigma_1, mu_2, sigma_2):\n",
    "        device = mu_1.device\n",
    "\n",
    "        mu_1 = mu_1.to(device)\n",
    "        mu_2 = mu_2.to(device)\n",
    "        sigma_1 = sigma_1.to(device)\n",
    "        sigma_2 = sigma_2.to(device)\n",
    "        \n",
    "        d = mu_1.numel()\n",
    "        kl = 0.5 * (torch.sum(torch.log(sigma_2) - torch.log(sigma_1)) * 2 +\n",
    "                    torch.sum(sigma_1**2 / sigma_2**2) +\n",
    "                    torch.sum((mu_2 - mu_1)**2 / sigma_2**2) - d)\n",
    "        return kl \n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_mu_sigma(stoch_network):\n",
    "        mu_net, sigma_net = [torch.cat(a) for a in \n",
    "                             zip(*[\n",
    "                                  [torch.cat([layer.weight_mu.flatten(), layer.bias_mu.flatten()]),\n",
    "                                  torch.cat([layer.weight_sigma.flatten(), layer.bias_sigma.flatten()])]\n",
    "                                  for \n",
    "                                  layer in stoch_network if\n",
    "                                  isinstance(layer, StochLinear)])\n",
    "                            ]\n",
    "        return mu_net, sigma_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.144063Z",
     "iopub.status.idle": "2025-03-19T11:29:11.144402Z",
     "shell.execute_reply": "2025-03-19T11:29:11.144263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_multiclass = MLP(nb_nodes=600, nb_hidden_layers=2, classes=10)\n",
    "state_dict = torch.load(\"model_init_multiclass.pt\")\n",
    "model_multiclass.load_state_dict(state_dict)\n",
    "model_multiclass.cuda()\n",
    "crtierion = torch.nn.CrossEntropyLoss()\n",
    "prior_mu = torch.cat([p.detach().flatten() for p in model_multiclass.layers.parameters()])\n",
    "pac_bound = PACBound(prior_mu, lambda_init=1e-3, likelihood_fn=crtierion, num_data=len(train_dataset))\n",
    "\n",
    "stoch_model_multi = StochaNN(nb_nodes=600, nb_hidden_layers=1, classes=10, isp_sigma=-2)\n",
    "for stoch_layer, layer in zip(stoch_model_multi.layers, model_multiclass.layers):\n",
    "    if isinstance(stoch_layer, StochLinear):\n",
    "        stoch_layer.copy_weights(layer)\n",
    "stoch_model_multi.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-19T11:29:11.145292Z",
     "iopub.status.idle": "2025-03-19T11:29:11.145619Z",
     "shell.execute_reply": "2025-03-19T11:29:11.145508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time as ts\n",
    "\n",
    "# Number of epochs to match the paper (200,000 iterations)\n",
    "\n",
    "epochs = 4\n",
    "initial_lr = 0.001  # First 150,000 iterations\n",
    "lr_drop_epoch = 250  # Drop learning rate at 150,000 iterations\n",
    "lr_factor = 0.1  # Reduce learning rate by lr_factor\n",
    "eval_freq = 2\n",
    "\n",
    "# Initialize RMSprop optimizer (with alpha=0.9 as in the paper)\n",
    "optimizer_sto = torch.optim.RMSprop([\n",
    "    {'params': stoch_model_multi.parameters(), 'lr': initial_lr, 'alpha': 0.9}, \n",
    "    {'params': pac_bound.log_lambda_, 'lr': initial_lr}\n",
    "])\n",
    "\n",
    "lambdas, train_accs, test_accs, b_re_values, kl_values, bounds = [], [], [], [], [], []\n",
    "idx = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    _ts = ts.perf_counter()\n",
    "\n",
    "    # Drop learning rate after 250 epochs\n",
    "    if epoch == lr_drop_epoch:\n",
    "        for param_group in optimizer_sto.param_groups:\n",
    "            param_group['lr'] *= lr_factor\n",
    "\n",
    "    # Run one training epoch\n",
    "    train_res = train_epoch(train_loader, stoch_model_multi, pac_bound.train_bound, optimizer_sto)\n",
    "    if epoch % eval_freq == 0:\n",
    "        idx.append(epoch)\n",
    "        test_res = eval_model(test_loader, stoch_model_multi, pac_bound.train_bound)\n",
    "        test_accs.append(test_res['stats']['accuracy'])\n",
    "        print(f'**** Epoch {epoch + 1} / {epochs}: Test accuracy: {test_accs[-1]} ****')\n",
    "    train_accs.append(train_res['stats']['accuracy'])\n",
    "    \n",
    "    # Compute PAC-Bayes metrics\n",
    "    lambdas.append(torch.exp(pac_bound.log_lambda_).item())\n",
    "    b_re_values.append(pac_bound.B_RE(stoch_model_multi).item())\n",
    "\n",
    "    stoch_model_multi.eval()\n",
    "    kl = pac_bound.kl(stoch_model_multi).item()\n",
    "    kl_values.append(kl)\n",
    "    bounds.append(get_approx_bound(b_re_values[-1], train_accs[-1]))\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} / {epochs}: Train Acc = {train_accs[-1] :.4f}, λ = {lambdas[-1] :.4f}, \"\n",
    "          f\"Bound:{bounds[-1] :.4f}, B_RE = {b_re_values[-1] :.4f}, KL = {kl_values[-1] :.4f}, Time: {ts.perf_counter() - _ts :.2f}s\")\n",
    "\n",
    "\n",
    "# Plots\n",
    "f, arr = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "arr[0].plot(train_accs, lw=2, label=\"Train Accuracy\", color=\"blue\")\n",
    "arr[0].plot(idx, test_accs, lw=2, label=\"Test Accuracy\", color=\"red\")\n",
    "arr[0].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "arr[0].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[0].legend()\n",
    "arr[0].grid()\n",
    "\n",
    "arr[1].plot(lambdas, lw=2, label=r\"$\\lambda$\", color=\"red\")\n",
    "arr[1].set_ylabel(r\"$\\lambda$\", fontsize=14)\n",
    "arr[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[1].legend()\n",
    "arr[1].grid()\n",
    "\n",
    "# Plot KL Divergence\n",
    "arr[2].plot(kl_values, lw=2, label=\"KL Divergence\", color=\"green\")\n",
    "arr[2].set_ylabel(\"KL Divergence\", fontsize=14)\n",
    "arr[2].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[2].legend()\n",
    "arr[2].grid()\n",
    "\n",
    "# Plot Bound\n",
    "arr[3].plot(bounds, lw=2, label=\"Bound\", color=\"green\")\n",
    "arr[3].set_ylabel(\"Bound\", fontsize=14)\n",
    "arr[3].set_xlabel(\"Epoch\", fontsize=14)\n",
    "arr[3].legend()\n",
    "arr[3].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NPM3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
